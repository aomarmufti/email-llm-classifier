{"cells":[{"source":"![email inbox](email_inbox.jpg)\n\nEvery day, professionals wade through hundreds of emails, from urgent client requests to promotional offers. It's like trying to find important messages in a digital ocean. But AI can help you stay afloat by automatically sorting emails to highlight what matters most.\n\nYou've been asked to build an intelligent email assistant using Llama, to help users automatically classify their incoming emails. Your system will identify which emails need immediate attention, which are regular updates, and which are promotions that can wait or be archived.\n\n### The Data\nYou'll work with a dataset of various email examples, ranging from urgent business communications to promotional offers. Here's a peek at what you'll be working with:\n\n### email_categories_data.csv\n\n Column | Description |\n|--------|-------------|\n| email_id | A unique identifier for each email in the dataset. |\n| email_content | The full email text including subject line and body. Each email follows a format of \"Subject\" followed by the message content on a new line. |\n| expected_category | The correct classification of the email: `Priority`, `Updates`, or `Promotions`. This will be used to validate your model's performance. |\n\n","metadata":{},"id":"4a1291c4-61f8-49ef-899f-89c178cdfd58","cell_type":"markdown"},{"source":"# Run the following cells first\n# Install necessary packages, then import the model running the cell below\n!pip install llama-cpp-python==0.3.7 -q -q -q","metadata":{"executionCancelledAt":null,"executionTime":23048,"lastExecutedAt":1750346961163,"lastExecutedByKernel":"9a19ecbb-1ab2-4351-a947-f3e28ac760e8","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Run the following cells first\n# Install necessary packages, then import the model running the cell below\n!pip install llama-cpp-python==0.3.7 -q -q -q","outputsMetadata":{"0":{"height":353,"type":"stream"}}},"id":"c398267d-bd03-485d-80af-db6d6d2529e4","cell_type":"code","execution_count":18,"outputs":[]},{"source":"SELECT *\nFROM 'models.csv'\nLIMIT 5","metadata":{"customType":"sql","dataFrameVariableName":"df","sqlCellMode":"dataFrame","sqlSource":{"integrationId":"c9696c24-44f3-45f7-8ccd-4b9b046e7e53","integrationType":"files","type":"integration"},"integrationExample":{"example":"-- Explore the data in the table\nSELECT *\nFROM 'models.csv'\nLIMIT 5","sqlSource":{"integrationId":"c9696c24-44f3-45f7-8ccd-4b9b046e7e53","integrationType":"files","type":"integration"}},"executionCancelledAt":null,"executionTime":58,"lastExecutedAt":1750346961221,"lastExecutedByKernel":"9a19ecbb-1ab2-4351-a947-f3e28ac760e8","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"SELECT *\nFROM 'models.csv'\nLIMIT 5","outputsMetadata":{"0":{"height":500,"type":"dataFrame","tableState":{}}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false}},"id":"bbe8a101-615a-4bd9-af1e-e4f6e69008a0","cell_type":"code","execution_count":19,"outputs":[{"output_type":"execute_result","data":{"application/com.datacamp.data-table.v2+json":{"table":{"schema":{"fields":[{"name":"index","type":"integer"},{"name":"column0","type":"string"},{"name":"column1","type":"string"},{"name":"column2","type":"string"}],"primaryKey":["index"],"pandas_version":"1.4.0"},"data":{"index":[0,1,2],"column0":["model","tinyllama-1.1b-chat-v0.3.Q4_K_M","llama-3.2-3b-instruct-q8_0"],"column1":["filepath","/files-integrations/files/c9696c24-44f3-45f7-8ccd-4b9b046e7e53/tinyllama-1.1b-chat-v0.3.Q4_K_M.gguf","/files-integrations/files/c9696c24-44f3-45f7-8ccd-4b9b046e7e53/llama-3.2-3b-instruct-q8_0.gguf"],"column2":["source","https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v0.3-GGUF/tree/main","https://huggingface.co/hugging-quants/Llama-3.2-3B-Instruct-Q8_0-GGUF/tree/main"]}},"total_rows":3,"truncation_type":null},"text/plain":"                           column0  ...                                            column2\n0                            model  ...                                             source\n1  tinyllama-1.1b-chat-v0.3.Q4_K_M  ...  https://huggingface.co/TheBloke/TinyLlama-1.1B...\n2       llama-3.2-3b-instruct-q8_0  ...  https://huggingface.co/hugging-quants/Llama-3....\n\n[3 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>column0</th>\n      <th>column1</th>\n      <th>column2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>model</td>\n      <td>filepath</td>\n      <td>source</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>tinyllama-1.1b-chat-v0.3.Q4_K_M</td>\n      <td>/files-integrations/files/c9696c24-44f3-45f7-8...</td>\n      <td>https://huggingface.co/TheBloke/TinyLlama-1.1B...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>llama-3.2-3b-instruct-q8_0</td>\n      <td>/files-integrations/files/c9696c24-44f3-45f7-8...</td>\n      <td>https://huggingface.co/hugging-quants/Llama-3....</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{"application/com.datacamp.data-table.v2+json":{"status":"success"}},"execution_count":19,"@datacamp/metadata":{"executedQuery":"SELECT *\nFROM 'models.csv'\nLIMIT 5","executedQueryParameters":[]}}]},{"source":"# Import required libraries\nimport pandas as pd\nfrom llama_cpp import Llama","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1750346961269,"lastExecutedByKernel":"9a19ecbb-1ab2-4351-a947-f3e28ac760e8","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import required libraries\nimport pandas as pd\nfrom llama_cpp import Llama"},"id":"6932fc36-bb8e-4ae3-86fd-e3a677a26b4e","cell_type":"code","execution_count":20,"outputs":[]},{"source":"# Load the email dataset\nemails_df = pd.read_csv('data/email_categories_data.csv')\n# Display the first few rows of our dataset\nprint(\"Preview of our email dataset:\")\nemails_df.head(2)","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1750346961321,"lastExecutedByKernel":"9a19ecbb-1ab2-4351-a947-f3e28ac760e8","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Load the email dataset\nemails_df = pd.read_csv('data/email_categories_data.csv')\n# Display the first few rows of our dataset\nprint(\"Preview of our email dataset:\")\nemails_df.head(2)","collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"outputsMetadata":{"0":{"height":38,"type":"stream"},"1":{"height":501,"type":"dataFrame","tableState":{"customFilter":{"const":{"type":"boolean","valid":true,"value":true},"id":"4ca4c6d9-90d3-41b7-aaf6-574e78938ebf","nodeType":"const"},"quickFilterText":""}}}},"id":"a5fea094-0942-4abb-aa0b-c78bb3d2543c","cell_type":"code","execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":"Preview of our email dataset:\n"},{"output_type":"execute_result","data":{"application/com.datacamp.data-table.v2+json":{"table":{"schema":{"fields":[{"name":"index","type":"integer"},{"name":"email_id","type":"integer"},{"name":"email_content","type":"string"},{"name":"expected_category","type":"string"}],"primaryKey":["index"],"pandas_version":"1.4.0"},"data":{"index":[0,1],"email_id":[1,2],"email_content":["Urgent: Server Maintenance Required\\nOur main server needs immediate maintenance due to critical errors. Please address ASAP.","50% Off Spring Collection!\\nDon't miss our biggest sale of the season! All spring items half off. Limited time offer."],"expected_category":["Priority","Promotions"]}},"total_rows":2,"truncation_type":null},"text/plain":"   email_id  ... expected_category\n0         1  ...          Priority\n1         2  ...        Promotions\n\n[2 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>email_id</th>\n      <th>email_content</th>\n      <th>expected_category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Urgent: Server Maintenance Required\\nOur main ...</td>\n      <td>Priority</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>50% Off Spring Collection!\\nDon't miss our big...</td>\n      <td>Promotions</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{"application/com.datacamp.data-table.v2+json":{"status":"success"}},"execution_count":21}]},{"source":"# Set the model path\nmodel_path = \"/files-integrations/files/c9696c24-44f3-45f7-8ccd-4b9b046e7e53/tinyllama-1.1b-chat-v0.3.Q4_K_M.gguf\"","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1750346961373,"lastExecutedByKernel":"9a19ecbb-1ab2-4351-a947-f3e28ac760e8","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Set the model path\nmodel_path = \"/files-integrations/files/c9696c24-44f3-45f7-8ccd-4b9b046e7e53/tinyllama-1.1b-chat-v0.3.Q4_K_M.gguf\""},"id":"19f4f7a8-088b-44c4-a62f-44af9e9c9152","cell_type":"code","execution_count":22,"outputs":[]},{"source":"# Start coding here\n# Use as many cells as you need\nllm = Llama(model_path=model_path)\n\n# Step 3: Define prompt template\nprompt = \"\"\"\nYou are an AI assistant. Classify the following email into one of these categories: Priority, Updates, Promotions.\n\nExample 1:\nemail_content = \"Important - Billing Issue. Your payment failed. Please update your billing details immediately.\"\nResponse: Priority\n\nExample 2:\nemail_content = \"Your new iPhone order has been processed.\"\nResponse: Updates\n\nExample 3:\nemail_content = \"10% sale now on all Apple items!\"\nResponse: Promotions\n\nNow classify this new email:\nemail_content = \"{email_text}\"\nResponse:\"\"\"\n\n# Step 4: Define the function\ndef process_message(llm, email_text, prompt):\n    input_prompt = prompt.format(email_text=email_text)\n    response = llm(input_prompt, max_tokens=5, temperature=0, stop=[\"Q:\", \"\\n\"])\n    return response['choices'][0]['text'].strip()\n\n# Step 5: Run on first two emails\nresult1 = process_message(llm, emails_df.iloc[0][\"email_content\"], prompt)\nresult2 = process_message(llm, emails_df.iloc[1][\"email_content\"], prompt)\nprint(f\"Result 1: {result1}\")\nprint(f\"Result 2: {result2}\")\n# iterate through test emails - Loop through the emails DataFrame to process each email and extract its content and expected category.\n\nsample_df = emails_df.head(2)  # Only first 2\n\nfor idx, row in sample_df.iterrows():\n    email_text = row[\"email_content\"]\n    predicted = process_message(llm, email_text, prompt)\n    print(f\"Email {idx+1} predicted category: {predicted}\")\n","metadata":{"executionCancelledAt":null,"executionTime":27059,"lastExecutedAt":1750346988432,"lastExecutedByKernel":"9a19ecbb-1ab2-4351-a947-f3e28ac760e8","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Start coding here\n# Use as many cells as you need\nllm = Llama(model_path=model_path)\n\n# Step 3: Define prompt template\nprompt = \"\"\"\nYou are an AI assistant. Classify the following email into one of these categories: Priority, Updates, Promotions.\n\nExample 1:\nemail_content = \"Important - Billing Issue. Your payment failed. Please update your billing details immediately.\"\nResponse: Priority\n\nExample 2:\nemail_content = \"Your new iPhone order has been processed.\"\nResponse: Updates\n\nExample 3:\nemail_content = \"10% sale now on all Apple items!\"\nResponse: Promotions\n\nNow classify this new email:\nemail_content = \"{email_text}\"\nResponse:\"\"\"\n\n# Step 4: Define the function\ndef process_message(llm, email_text, prompt):\n    input_prompt = prompt.format(email_text=email_text)\n    response = llm(input_prompt, max_tokens=5, temperature=0, stop=[\"Q:\", \"\\n\"])\n    return response['choices'][0]['text'].strip()\n\n# Step 5: Run on first two emails\nresult1 = process_message(llm, emails_df.iloc[0][\"email_content\"], prompt)\nresult2 = process_message(llm, emails_df.iloc[1][\"email_content\"], prompt)\nprint(f\"Result 1: {result1}\")\nprint(f\"Result 2: {result2}\")\n# iterate through test emails - Loop through the emails DataFrame to process each email and extract its content and expected category.\n\nsample_df = emails_df.head(2)  # Only first 2\n\nfor idx, row in sample_df.iterrows():\n    email_text = row[\"email_content\"]\n    predicted = process_message(llm, email_text, prompt)\n    print(f\"Email {idx+1} predicted category: {predicted}\")\n","outputsMetadata":{"0":{"height":523,"type":"stream"},"1":{"height":59,"type":"stream"},"2":{"height":122,"type":"stream"},"3":{"height":38,"type":"stream"},"4":{"height":101,"type":"stream"},"5":{"height":38,"type":"stream"}}},"id":"7ade0f6f-e0db-415d-b906-e9820370b34e","cell_type":"code","execution_count":23,"outputs":[{"output_type":"stream","name":"stderr","text":"llama_model_loader: loaded meta data with 20 key-value pairs and 201 tensors from /files-integrations/files/c9696c24-44f3-45f7-8ccd-4b9b046e7e53/tinyllama-1.1b-chat-v0.3.Q4_K_M.gguf (version GGUF V2)\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.name str              = py007_tinyllama-1.1b-chat-v0.3\nllama_model_loader: - kv   2:                       llama.context_length u32              = 2048\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 2048\nllama_model_loader: - kv   4:                          llama.block_count u32              = 22\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 5632\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 64\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 4\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\nllama_model_loader: - kv  11:                          general.file_type u32              = 15\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32003]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32003]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32003]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  19:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   45 tensors\nllama_model_loader: - type q4_K:  135 tensors\nllama_model_loader: - type q6_K:   21 tensors\nprint_info: file format = GGUF V2\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 636.18 MiB (4.85 BPW) \ninit_tokenizer: initializing tokenizer for type 1\nload: control-looking token:  32002 '<|im_end|>' was not control-type; this is probably a bug in the model. its type will be overridden\nload: control token:      2 '</s>' is not marked as EOG\nload: control token:      1 '<s>' is not marked as EOG\nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: special tokens cache size = 6\nload: token to piece cache size = 0.1684 MB\nprint_info: arch             = llama\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 2048\nprint_info: n_embd           = 2048\nprint_info: n_layer          = 22\nprint_info: n_head           = 32\nprint_info: n_head_kv        = 4\nprint_info: n_rot            = 64\nprint_info: n_swa            = 0\nprint_info: n_embd_head_k    = 64\nprint_info: n_embd_head_v    = 64\nprint_info: n_gqa            = 8\nprint_info: n_embd_k_gqa     = 256\nprint_info: n_embd_v_gqa     = 256\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-05\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: n_ff             = 5632\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 0\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 10000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 2048\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = 1B\nprint_info: model params     = 1.10 B\nprint_info: general.name     = py007_tinyllama-1.1b-chat-v0.3\nprint_info: vocab type       = SPM\nprint_info: n_vocab          = 32003\nprint_info: n_merges         = 0\nprint_info: BOS token        = 1 '<s>'\nprint_info: EOS token        = 2 '</s>'\nprint_info: EOT token        = 32002 '<|im_end|>'\nprint_info: UNK token        = 0 '<unk>'\nprint_info: LF token         = 13 '<0x0A>'\nprint_info: EOG token        = 2 '</s>'\nprint_info: EOG token        = 32002 '<|im_end|>'\nprint_info: max token length = 48\nload_tensors: layer   0 assigned to device CPU\nload_tensors: layer   1 assigned to device CPU\nload_tensors: layer   2 assigned to device CPU\nload_tensors: layer   3 assigned to device CPU\nload_tensors: layer   4 assigned to device CPU\nload_tensors: layer   5 assigned to device CPU\nload_tensors: layer   6 assigned to device CPU\nload_tensors: layer   7 assigned to device CPU\nload_tensors: layer   8 assigned to device CPU\nload_tensors: layer   9 assigned to device CPU\nload_tensors: layer  10 assigned to device CPU\nload_tensors: layer  11 assigned to device CPU\nload_tensors: layer  12 assigned to device CPU\nload_tensors: layer  13 assigned to device CPU\nload_tensors: layer  14 assigned to device CPU\nload_tensors: layer  15 assigned to device CPU\nload_tensors: layer  16 assigned to device CPU\nload_tensors: layer  17 assigned to device CPU\nload_tensors: layer  18 assigned to device CPU\nload_tensors: layer  19 assigned to device CPU\nload_tensors: layer  20 assigned to device CPU\nload_tensors: layer  21 assigned to device CPU\nload_tensors: layer  22 assigned to device CPU\nload_tensors: tensor 'token_embd.weight' (q4_K) (and 200 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\nload_tensors:   CPU_Mapped model buffer size =   636.18 MiB\nllama_init_from_model: n_seq_max     = 1\nllama_init_from_model: n_ctx         = 512\nllama_init_from_model: n_ctx_per_seq = 512\nllama_init_from_model: n_batch       = 512\nllama_init_from_model: n_ubatch      = 512\nllama_init_from_model: flash_attn    = 0\nllama_init_from_model: freq_base     = 10000.0\nllama_init_from_model: freq_scale    = 1\nllama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (2048) -- the full capacity of the model will not be utilized\nllama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 22, can_shift = 1\nllama_kv_cache_init: layer 0: n_embd_k_gqa = 256, n_embd_v_gqa = 256\nllama_kv_cache_init: layer 1: n_embd_k_gqa = 256, n_embd_v_gqa = 256\nllama_kv_cache_init: layer 2: n_embd_k_gqa = 256, n_embd_v_gqa = 256\nllama_kv_cache_init: layer 3: n_embd_k_gqa = 256, n_embd_v_gqa = 256\nllama_kv_cache_init: layer 4: n_embd_k_gqa = 256, n_embd_v_gqa = 256\nllama_kv_cache_init: layer 5: n_embd_k_gqa = 256, n_embd_v_gqa = 256\nllama_kv_cache_init: layer 6: n_embd_k_gqa = 256, n_embd_v_gqa = 256\nllama_kv_cache_init: layer 7: n_embd_k_gqa = 256, n_embd_v_gqa = 256\nllama_kv_cache_init: layer 8: n_embd_k_gqa = 256, n_embd_v_gqa = 256\nllama_kv_cache_init: layer 9: n_embd_k_gqa = 256, n_embd_v_gqa = 256\nllama_kv_cache_init: layer 10: n_embd_k_gqa = 256, n_embd_v_gqa = 256\nllama_kv_cache_init: layer 11: n_embd_k_gqa = 256, n_embd_v_gqa = 256\nllama_kv_cache_init: layer 12: n_embd_k_gqa = 256, n_embd_v_gqa = 256\nllama_kv_cache_init: layer 13: n_embd_k_gqa = 256, n_embd_v_gqa = 256\nllama_kv_cache_init: layer 14: n_embd_k_gqa = 256, n_embd_v_gqa = 256\nllama_kv_cache_init: layer 15: n_embd_k_gqa = 256, n_embd_v_gqa = 256\nllama_kv_cache_init: layer 16: n_embd_k_gqa = 256, n_embd_v_gqa = 256\nllama_kv_cache_init: layer 17: n_embd_k_gqa = 256, n_embd_v_gqa = 256\nllama_kv_cache_init: layer 18: n_embd_k_gqa = 256, n_embd_v_gqa = 256\nllama_kv_cache_init: layer 19: n_embd_k_gqa = 256, n_embd_v_gqa = 256\nllama_kv_cache_init: layer 20: n_embd_k_gqa = 256, n_embd_v_gqa = 256\nllama_kv_cache_init: layer 21: n_embd_k_gqa = 256, n_embd_v_gqa = 256\nllama_kv_cache_init:        CPU KV buffer size =    11.00 MiB\nllama_init_from_model: KV self size  =   11.00 MiB, K (f16):    5.50 MiB, V (f16):    5.50 MiB\nllama_init_from_model:        CPU  output buffer size =     0.12 MiB\nllama_init_from_model:        CPU compute buffer size =    66.51 MiB\nllama_init_from_model: graph nodes  = 710\nllama_init_from_model: graph splits = 1\nCPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \nModel metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '2048', 'general.name': 'py007_tinyllama-1.1b-chat-v0.3', 'llama.embedding_length': '2048', 'llama.feed_forward_length': '5632', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '64', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '22', 'llama.attention.head_count_kv': '4', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\nUsing fallback chat format: llama-2\nllama_perf_context_print:        load time =   15230.00 ms\nllama_perf_context_print: prompt eval time =   15229.69 ms /   167 tokens (   91.20 ms per token,    10.97 tokens per second)\nllama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\nllama_perf_context_print:       total time =   15230.23 ms /   168 tokens\nLlama.generate: 135 prefix-match hit, remaining 32 prompt tokens to eval\nllama_perf_context_print:        load time =   15230.00 ms\nllama_perf_context_print: prompt eval time =    1901.88 ms /    32 tokens (   59.43 ms per token,    16.83 tokens per second)\nllama_perf_context_print:        eval time =     310.82 ms /     3 runs   (  103.61 ms per token,     9.65 tokens per second)\nllama_perf_context_print:       total time =    2214.29 ms /    35 tokens\nLlama.generate: 135 prefix-match hit, remaining 32 prompt tokens to eval\n"},{"output_type":"stream","name":"stdout","text":"Result 1: \nResult 2: Promotions\n"},{"output_type":"stream","name":"stderr","text":"llama_perf_context_print:        load time =   15230.00 ms\nllama_perf_context_print: prompt eval time =    1982.99 ms /    32 tokens (   61.97 ms per token,    16.14 tokens per second)\nllama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\nllama_perf_context_print:       total time =    1983.50 ms /    33 tokens\nLlama.generate: 135 prefix-match hit, remaining 32 prompt tokens to eval\n"},{"output_type":"stream","name":"stdout","text":"Email 1 predicted category: \n"},{"output_type":"stream","name":"stderr","text":"llama_perf_context_print:        load time =   15230.00 ms\nllama_perf_context_print: prompt eval time =    6897.96 ms /    32 tokens (  215.56 ms per token,     4.64 tokens per second)\nllama_perf_context_print:        eval time =     312.23 ms /     3 runs   (  104.08 ms per token,     9.61 tokens per second)\nllama_perf_context_print:       total time =    7211.40 ms /    35 tokens\n"},{"output_type":"stream","name":"stdout","text":"Email 2 predicted category: Promotions\n"}]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"editor":"DataLab"},"nbformat":4,"nbformat_minor":5}